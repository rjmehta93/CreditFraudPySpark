# LOGISTIC REGRESSION
import findspark
findspark.init()
import time
from pyspark.sql import SQLContext
from pyspark import SparkContext
from pyspark.sql import SparkSession
spark = SparkSession.builder.config("spark.sql.shuffle.partitions", "50").getOrCreate()
sc = spark.sparkContext.setLogLevel("WARN")
df = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('creditcard.csv')


startTimeQuery = time.clock()
from pyspark.sql.types import *
def convertColumn(df, names, newType):
  for name in names: 
     df = df.withColumn(name, df[name].cast(newType))
  return df 
columns = ['Time','V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28','Amount','Class']
df = convertColumn(df, columns, FloatType())

#df.printSchema()

from pyspark.sql.functions import *
df = df.select('Class','Amount','Time','V1','V2','V3','V4','V5','V6','V7','V8','V9','V10','V11','V12','V13','V14','V15','V16','V17','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28')



from pyspark.ml.linalg import DenseVector

# Define the `input_data` 
input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))

# Replace `df` with the new DataFrame
df = spark.createDataFrame(input_data, ["label", "features"])



from pyspark.ml.feature import StandardScaler

# Initialize the `standardScaler`
standardScaler = StandardScaler(inputCol="features", outputCol="features_scaled")

# Fit the DataFrame to the scaler
scaler = standardScaler.fit(df)

# Transform the data in `df` with the scaler
scaled_df = scaler.transform(df)



from pyspark.sql.functions import rand 
df = df.orderBy(rand())
train_data, test_data = df.randomSplit([0.8, 0.2],seed=1234)



from pyspark.ml.classification import LogisticRegression


mlr = LogisticRegression(labelCol="label", featuresCol="features", maxIter=1)

# Fit the model
mlrModel = mlr.fit(train_data)



predicted = mlrModel.transform(test_data)

predictions = predicted.select("prediction").rdd.map(lambda x: x[0])
labels = predicted.select("label").rdd.map(lambda x: x[0])


b = predictions.collect()
a = labels.collect()


if len(a)>len(b):
    c = len(b)
else:
    c = len(a)

tp=0
for i in range(0,c):
    if a[i] == 0.0 and b[i] == 0.0:
        tp += 1
print("Correct Legit Case:",tp)

tn=0
for i in range(0,c):
    if a[i] == 1.0 and b[i] == 1.0:
        tn += 1
print("Correct Fraud Case:",tn)

fp=0
for i in range(0,c):
    if a[i] == 0 and b[i] == 1:
        fp += 1
print("False Fraud Case:",fp)

fn=0
for i in range(0,c):
    if a[i] == 1 and b[i] == 0:
        fn += 1
print("False Legit Case:",fn)


r = float(tp)/(tp + fn)
print ("recall", r)

p = float(tp) / (tp + fp)
print ("precision", p)


endTimeQuery = time.clock()
runTimeQuery = endTimeQuery - startTimeQuery
print(runTimeQuery)
